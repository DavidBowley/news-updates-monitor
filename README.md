# News Updates Monitor

## Introduction
This was my first major coding project since working my way through the ThinkPython course material. It was designed specifically to work for me so YMMV. As with any scraping project there is a shelf-life on how long the parsing logic will stay relevant, so by the time you read this it may already be dead in the water...

The reason this code exists is due to me noticing a number of grammatical errors on certain BBC News articles and wondering whether anyone ever fixed them. I also found some of the headlines hilarious (probably for inadvertent reasons) and I always wondered if in the future they would be changed because someone saw it in the same light as myself.

So the News Updates Monitor was born...

It consists of a back-end where all the heavy lifting occurs, and a front-end Flask app that showcases the data from the SQLite3 database.

## What Does It Do?
It is designed to specifically target _only_ BBC News articles – _not_ Sports news, _not_ Live update articles, just URLs that end in ‘/news/article/[id]’

It checks for any new news articles from the news homepage, and begins to track them once added to the system. It continues to check for updates on old articles based on a schedule:
* Up to 3 hours old: checks every run (approx. every 15-20 minutes)
* Between 3 and 24 hours old: checks every hour
* Between 24 and 48 hours old: checks every 8 hours
* Between 48 hours and 1 week old: checks every day
* Between 1 and 4 weeks old: checks every week
* Over 4 weeks old: checks every 4 weeks

If there are no changes detected, it simply logs the article’s fetch history and does nothing else. If there are changes in the article content (after parsing out the headline, body, timestamp, and byline sections) then the new content is recorded for later comparisons.

## Comparing Articles
The front-end allows a user to view an individual article’s different versions and compare them to each other visually using the Python’s difflib module, so it is very obvious which words/sentences have been added, removed, or edited.

This is helped by the parsing logic which tries to maintain as much of the HTML generated by the BBC content management system as possible – so we keep all the paragraph tags but remove all their CSS classes. This means the system can tell even if an editor simply added a new empty paragraph in order to add an extra line break, and makes the results pretty accurate.

It’s also interesting to note the published timestamp (available in the article's <time> HTML tag as an ISO8601 datetime) because this is only updated when a content editor requests it. Therefore changes can be made to articles with no public acknowledgement, or even previous updates to the official timestamp can be overridden by a future content editor – all of these changes are captured by the system (in as much as it is possible with the scheduling setup – obviously multiple changes made in between article fetches cannot be recorded).

## Dependencies
Needs Python 3.12.4 or thereabouts - can't remember exactly how much lower it can get but it definitely needs the 3.11 datetime.fromisoformat().

There’s a bunch of dependencies (including my fork of an abandoned library) in [Requirements.txt](/requirements.txt) , and the [db_schema.sql](/db_schema.sql) file has everything needed to setup the SQLite tables.

## Limitations
This was intended as a quick "intro to Python" for myself, and wasn't designed for others to use, so it may not be the most intuitive!

It also _really_ doesn't like downtime. I left it offline for a week and it had queued up over 2,000 articles (which would all have the same scheduling slots, i.e. 2,000 at the same time day or week etc.) To get around this I added a schedule_level 0 - if this ever happens just manually update the article's schedule_level in the Tracking table and it will be ignored for future runs.

## Running 24/7
The script should be run 24/7. I used an old Raspberry Pi but any old Linux server should be good enough. I recommend [tmux](https://github.com/tmux/tmux/wiki) for keeping the script alive, then you can just SSH in to keep an eye on it as and when you feel like it. I've had it running for 4 weeks without any memory issues with only 1GB of RAM, but I suspect as the number of recurring fetches increases this would eventually have performance issues.

## Proxies?
Surprisingly no. It runs every 15-20 minutes, but only ever makes one request per 5 second period. Despite racking up a few GB on the same IP I've not had the need for proxies yet.

## Screenshots

### Homepage

![Screenshot of the homepage showing a table of BBC News article URLs along with their number of article snapshots recorded. The pagination shows there are 26 pages of 100 articles per page.](/screenshots/home_full.png)


### Article Details

![Screenshot of the Article Details page showing the most recent data avaialble for the example article, such as headline, byline, timestamp, number of snapshots, number of fetches, and its current schedule level. It also shows links to the pages where adjacent versions of the same article can be compared.](/screenshots/article.png)

### Fetch History

![Screenshot of the Fetch History page showing all the fetches (HTTP requests) for a particular article, the HTTP status code returned, and the schedule level recorded at the time of the fetch.](/screenshots/fetch_history2.png)

### Article Comparison

![Screenshot of the Article Comparison page showing an example of the Python difflib comparison, which uses color-coding to highlight specific changes made between the versions.](/screenshots/article_comparison.png)
